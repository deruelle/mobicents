<?xml version='1.0'?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [
]>
<section
	id="section-Media_Server_Architecture">
	<title>Media Server Architecture</title>
	<para>Media services have played an important role in the traditional Time Division Multiplexing (<acronym>TDM</acronym>)-based telephone network. As the network migrates to an Internet Protocol (<acronym>IP</acronym>)-based environment, media services are also moving to new environments.</para>
	<para>One of the most exciting trends is the emergence and adoption of complementary modular standards that leverage the Internet to enable media services to be developed, deployed and updated more rapidly than before in a network architecture that supports the two concepts we will call provisioning-on-demand and scaling-on-demand.</para>
	<!-- <para>Media services have played an important role in the traditional time division multiplexing (TDM)-based telephone network. As the network migrates to an Internet Protocol (IP)-based environment, media services are also moving to new environments.  One of the most exciting trends is the emergence and adoption of complementary modular standards that leverage the Internet to enable media services to be developed, deployed, and updated more rapidly than before in a network architecture that supports concepts we call provisioning on demand and scaling on demand.</para> -->
	<formalpara>
		<title>General Design Overview</title>
		<para>Mobicents Media Server is developed on top of existing Java technologies. The Java platform is the ideal platform for network computing. It offers a single, unifying programming model that can connect all elements of a business infrastructure. The modularization effort is supported by the use of the Java Management Extension (<acronym>JMX</acronym>) <acronym>API</acronym>, and the industry-standard Service Logic Execution Environment (<acronym>SLEE</acronym>) container. Using JMX enables easy management of both the server's media components and the control modules hosted by <acronym>SLEE</acronym>.</para>
	</formalpara>
 <!-- 
General disign overview.
 
Mobicents Media server is developed on top of the existing Java Techologies. The Java platform is the ideal platform for network computing. It offers a single, unifying programming model that can connect all elements of a business infrastructure. The modularization effort is supported by the use of JMX, Java Management Extension API and the industry standard SLEE container. Using JMX help manage both server's media components and control modules hosted by SLEE. -->
	<para>This high degree of modularity benefits the application developer in several ways. The already-tight code can be further trimmed down to support applications that must have small footprints. For example, if <acronym>PSTN</acronym> (Public Switched Telephone Network) interconnection is unnecessary in your application, then simply take the D-channel feature out of the server. If you later decide to deploy the same application within a Signaling System 7 (<acronym>SS7</acronym>) network, then simply enable the appropriate endpoint. Another example is the freedom you have to drop your favorite media control protocol directly into the <acronym>SLEE</acronym> container.</para>
<!--
This hi-degree of modularity benefits the application developer in several ways. The already tight code can be further trimmed down to support application that must have a small footprint. For example if PSTN interconnection is unnecessary in your application simply take the D-channel feature out of the server. If you later decide to deploy same application withing an SS7 network simply enable the appropriate endpoint. Another example is the freedom you have to drop your favorite Media Control Protocol directly into the SLEE container.-->
	<figure
		id="figure-ms-general.jpg">
		<title>Media Server Overview</title>
		<mediaobject>
			<imageobject>
				<imagedata
					align="center"
					fileref="../../images/ms-general.jpg"
					format="JPG"/>
			</imageobject>
		</mediaobject>
	</figure>
	<para>The components that are involved in media processing or endpoints are implemented with <acronym>JMF</acronym>, the Java Media Framework. JMF provides a platform-neutral framework for displaying time-sensitive media. The <acronym>JMF</acronym> <acronym>API</acronym>:</para>
	<itemizedlist>
		<listitem>
			<para>Scales across different protocols and delivery mechanisms</para>
		</listitem>
		<listitem>
			<para>Scales across different types of media data</para>
		</listitem>
		<listitem>
			<para>Provides an event model for asynchnronous communication between JMF and applications or applets</para>
		</listitem>
	</itemizedlist>
	<para>The <acronym>JMF</acronym> architecture allows advanced developers nto create and integrate new types of controllers and data sources. The <acronym>JMF</acronym> <acronym>API</acronym> declares abstract interfaces which are used for handling new media sources and sinks such as audio and video files, <acronym>PSTN</acronym> cards, webcams, etc.</para>
	<para>The Media Server architecture assumes that call control intelligence is outside of the Media Server and handled by an external entity. The Mebia Server assumes that call controllers will use control procedures such as <acronym>MGCP</acronym>, Mecago or <acronym>MSML</acronym>, among others. Each specific control module can be plugged in directly to the server as a standard <acronym>SLEE</acronym> deployable unit. The usage of a <acronym>SLEE</acronym> container for the implementation of the control protocol-specific communication logic provides simple deployment and also the easy deployment of such control modules. The developer will not have to mess with low-level transaction and state management details, multi-threading, connection-pooling and other similar complex, low-level <acronym>API</acronym>s.</para>
	<note>
		<para>The Mobicents Media Server uses <acronym>SLEE</acronym> for implementing its own communication capabilities. The <acronym>SLEE</acronym> container doesn't serve here as a call controller.</para>
	</note>

<!--The components that are involved into media processing or Endpoints are implemented with JMF, Java Media Framework. JMF provides a platform neutral framework for displaing time-based media. The JMF API:

    * Scales across different protocols and delivery mechanisms
    * Scales across different types of media data
    * Provides an event model for asynchronous communication between JMF and applications or applets.

The JMF architecture allows advanced developers to create and integrate new types of controllers and data sources. JMF API declares abstract interfaces which used for handling new media sources or sinks like audio/video files, PSTN cards, web cams, etc.
 
Media server architecture assumes that call controll intelligence is outside of the Media Server and handled by external entity. Media Server assumes that call controllers will use any of controll protocols such as MGCP, Mecago, MSML or others. Each specific control module can be pluged-in directly to the server as standard SLEE deployable unit. The usage of SLEE container for the implementation of the control protocol specific communication logic provides simple development and deployment of such control modules. The developer will not have to low-level transaction and state management details, multi-threading, connection pooling and other low level complex APIs.
 
Note. Media Server uses SLEE for implementing its own communication capabilities. SLEE container doesn't serve here as call controller.-->
	<para>In addition to control protocol modules, the <acronym>SLEE</acronym> container is aimed at providing high-level features like Interactive Voice Response (<acronym>IVR</acronym>) and Drools or VoiceXML engines.</para>
<!--
In addition to control protocol modules the SLEE container is aimed to provide high-level features like IVR &amp; Drools or VoiceXML  engines.
-->
	<para>The mobdules deployed under <acronym>SLEE</acronym> control interact with the Media Server Service Provider Interface (<acronym>SPI</acronym>) through the Media Server Control Resource Adapter, or <acronym>MSC-RA</acronym>. The <acronym>MSC-RA</acronym> follows the recommendations of JSR-309 and implements asynchronous interconnection with the Media Server <acronym>SPI</acronym> stack. This local implementation is restricted and does not all the use of high-level abstractions like VoiceXML dialogs, <abbrev>etc.</abbrev>.</para>
<!--
The modules deployed under SLEE control interact with Media Server SPI though Media Server Control Resource Adapter. The MSC-RA follows to recomndations of JSR-309 and implements asynchronous interconnection with Media Server SPI stack. This local implementation is restricted and does not allow to use high level abstraction like Voice XML dialogs, etc.
-->
	<formalpara>
		<title>Typical Deployment Scenario</title>
		<para>Mobicents Media Server offers a complete solution for all aspects of being a media gateway and server. This includes the Digital Signal Processors required to covert and compress <acronym>TDB</acronym> voice circuits into <acronym>IP</acronym> packets, announcement access points, conferencing, high-level IVR engines, <abbrev>etc.</abbrev> The gateway is able to provide signaling conversation and can operate as a Session Border Controller at the boundaries of Local Access Networks. The Media Server is always controlled by an external <acronym>JBCP</acronym> application server which implements the call control logic.</para>
	</formalpara>
<!--
Typical deployment scenario.
 
Mobicent Media Server offers a complete solution for all aspects of media gateway and media server. This is include the Digital Signal Processors required to convert and compress TDM voice circuits into IP packets, Announcement access points, Conferencing, hi-level IVR engines, etc. The gateway is able to provide signalling conversation and can operate as Session Border Controller at the boundaries of the LANs.  The Media Server is always controlled by external JBCP Application server which implements the call control logic.
-->
	<formalpara>
		<title>Endpoints</title>
		<para>It is convenient to consider a media gateway as a collection of endpoints. An endpoint is a logical representation of a physical entity such as an analog phone or a channel in a trunk. Endpoints are sources or sinks of data and can be physical or virtual. Physical endpoint creation requires hardware installation whil esoftware is sufficient for creating a virtual endpoint. An interface on a gateway that terminates a trunk connected to a PSTN switch is an example of a physical endpoint. An audio source in an audio content server is an example of a virtual endpoint.</para>
	</formalpara>
<!--
3.1 Endpoints. 
 
It is convinient to consider media gateway as a collection of endpoints. An Endpoint is a logical representation of a physical entity, such as an analog phone or a channel in a trunk. Endpoints are sources or sinks of data and can be physical or virtual. Physical Endpoint creation requires hardware installation while software is sufficient for creating a virtual Endpoint. An interface on a gateway that terminates a trunk connected to a PSTN switch is an example of a physical Endpoint. An audio source in an audio-content server is an example of a virtual Endpoint.-->
	<para>The type of the endpoint determines its functionality. Our analysis, so far, has led us to isolate the following basic endpoint types:</para>
	<itemizedlist>
		<listitem>
			<para>Digital Signal 0 (<acronym>DS0</acronym>)</para>
		</listitem>
		<listitem>
			<para>Analog line</para>
		</listitem>
		<listitem>
			<para>Announcement server access point</para>
		</listitem>
		<listitem>
			<para>Conference bridge acces point</para>
		</listitem>
		<listitem>
			<para>Packet relay</para>
		</listitem>
		<listitem>
			<para>Asynchronous Transfer Mode (<acronym>ATM</acronym>) &quot;trunk side&quot; interface</para>
		</listitem>
	</itemizedlist>
<!--
The type of the endpoint determines its functionality. Our analysis,so far,has led us to isolate the following basic endpoint types:

    * Digital channel (DS0),
    * Analog line,
    * Announcement server access point,
    * Interactive Voice Response access point,
    * Conference bridge access point,
    * Packet relay,
    * ATM "trunk side"interface. -->
	<para>This list is not exhaustive: there may be other types of endpoints defined in the future, for example test endpoints that could be used to check network quality, or frame-relay endpoints that could be used to manage audio channels multiplexed over a frame-relay virtual circuit.</para>
<!--
This list is not final.There may be other types of endpoints defined in the future,for example test endpoints that could be used to check network quality,or frame-relay endpoints that could be used to manage audio channels multiplexed over a frame-relay virtual circuit.
-->
	<variablelist>
		<title>Description of Various Access Point Types</title>
		<varlistentry>
			<term>Announcement Server Access Point</term>
			<listitem>
				<para>An announcement server endpoint naturally provides access to an announcement server. Upon receiving requests from the call agent, the announcement server will &quot;play&quot; a specified announcement. A given announcement endpoint is not expected to support more than one connection at a time. Connections to an announcement server are typically one-way, or &quot;half-duplex&quot;:the announcement server is not expected to listen to the audio signals from the connection.</para>
<!--			Announcement Server Access Point

An announcement server endpoint provides access to an announcement service.  Under requests from the Call Agent, the announcement server will "play" a specified announcement.  A given announcement endpoint is not expected to support more than one connection at a time. Connections to an announcement server are typically one way, or "half  duplex" - the announcement server is not expected to listen to the audio signals from the connection.-->
			</listitem>
		</varlistentry>
		<varlistentry>
			<term>Interactive Voice Response Access Point</term>
			<listitem>
				<para>An Interactive Voice Repsonse (<acronym>IVR</acronym>) endpoint provides access to an <acronym>IVR</acronym> service. Upon requests from the call agent, the <acronym>IVR</acronym> server will &quot;play&quot; announcements and tones, and will &quot;listen&quot; to responses, such as (<acronym>DTMF</acronym>) input or voice messages, from the user. A given <acronym>IVR</acronym> endpoint is not expected to support more than one connection at a time.</para>
<!--			Interactive Voice Response Access Point

An Interactive Voice Response (IVR) endpoint provides access to an IVR service.  Under requests from the Call Agent, the IVR server will "play" announcements and tones, and will "listen" to responses, such as DTMF input or voice messages, from the user. A given IVR endpoint is not expected to support more than one connection at a time.-->
			</listitem>
		</varlistentry>
		<varlistentry>
			<term>Conference Bridge Access Point</term>
			<listitem>
				<para>A conference bridge endpoint is used to provide access to a specific conference. Media gateways should be able to establish several connections between the endpoint and the packet networks, or between the endpoint and other endpoints in the same gateway. The signals originating from these connections shall be mixed according to the connection &quot;mode&quot; (as specified later in this document). The precise number of connections that an endpoint supports is characteristic of the gateway, and may in fact vary according to the alloction of resources within the gateway.</para>
<!--			Conference Bridge Access Point

A conference bridge endpoint is used to provide access to a specific conference. Media gateways should be able to establish several connections between the endpoint and the packet networks, or between the endpoint and other endpoints in the same gateway.  The signals originating from these connections shall be mixed according to the connection "mode", as specified later in this document.  The precise number of connections that an endpoint supports is a characteristic of the gateway, and may in fact vary according to the allocation of resources within the gateway.-->
			</listitem>
		</varlistentry>
		<varlistentry>
			<term>Packet Relay</term>
			<listitem>
				<para>A packet relay endpoint is a specific form of conference bridge that typically only supports two connections. Packet relays can be found in firewalls between a protected and an open network, or in transcoding servers used to provide interoperation between incompatible gateways, such as gateways which don't support compatible compression algorithms, or gateways that operate over different transmission networks such as IP and ATM.</para>
<!--			Packet Relay

A packet relay endpoint is a specific form of conference bridge, that typically only supports two connections.  Packets relays can be found in firewalls between a protected and an open network, or in transcoding servers used to provide interoperation between   incompatible gateways, for example gateways that do not support compatible compression algorithms, or gateways that operate over  different transmission networks such as IP and ATM.-->
			</listitem>
		</varlistentry>
	</variablelist>
	<formalpara>
		<title>Signal Generators (<acronym>SG</acronym>s) and Signal Detectors (<acronym>SD</acronym>s)</title>
		<para>This endpoint contains a set of resources which provide the media-processing functionality. It manages the interconnection of media streams between the resources, and arbitrates the flow of media stream data between them. Media services&mdash;or commands&mdash; are invoked by a client application on the endpoint; that endpoint causes the resources to perform the desired services, and directs events sent by the resources to the appropriate client. The resources in the endpoint include a primary resource and zero or more secondary resources. The primary resource is typically connected to an external media stream, and provides the data from that stream to the secondary resources. The secondary resources may process that stream (<abbrev>e.g.</abbrev> by recording it and/or performing automatic speech recognition on it), or may themselves generate generate media stream data (<abbrev>e.g.</abbrev> by playing a voice file) which is then transmitted to the primary resource.</para>
	</formalpara>
	<figure
		id="figure-ms.png">
		<title></title>
		<mediaobject>
			<imageobject>
				<imagedata
					align="center"
					fileref="../../images/ms.png"
					format="PNG"/>
			</imageobject>
		</mediaobject>
	</figure>
<!--		
3.1.1 Signal Generators (SG) and Signal Detectors (SD)

The Endpoint contains a set of resources, which provide the media processing functionality. It manages the interconnection of media streams between the resources, and arbitrates the flow of media stream data between them. Media services (or commands) are invoked by a client application on the Endpoint; the endpoint causes the resources to perform the desired services, and directs events sent by the resources to the appropriate client. The resources in the Endpoint include a primary resource and zero or more secondary resources. The primary resource is typically connected to an external media stream, and provides the data from that stream to the secondary resources. The secondary resources may process that stream (e.g., by recording it, performing ASR on it), or may themselves generate media stream data (e.g., by playing a voice file) which is transmitted to the primary resource.-->
	<para>A resource is statically prepared if the preparation takes place at the time of creation. A resource is dynamically prepared if preparation of a particular resource (and its associated media streams) does not occur until it is required by a media operation. Static preparation can lead to less efficient usage of a server's resources, because resources may tend to be allocated for a longer time before use. However, once a resource has been prepared, it is guaranteed to be available for use. Dynamic preparation may utilize resources more efficiently because of Just-In-Time (<acronym>JIT</acronym>) allocation algorithms may be used.</para>
<!--A resource is statically prepared if preparation takes place at creation time. A resource is dynamically prepared if preparation of a particular resource (and its associated media streams) does not occur until it is required by a media operation. Static preparation can lead to less efficient usage of a serverâ€™s resources, because resources may tend to be allocated for a longer time before use. However, once a resource has been prepared, it is guaranteed to be available for use. Dynamic preparation may utilize resource more efficiently, because just-in-time (JIT) allocation algorithms may be used.-->
	<para>An endpoint is divided logically into a Service Provider Interface (<acronym>SPI</acronym>) that is used to implement specific a endpoint, and a management interface which is used for implementing manageable resources of that endpoint. All endpoints are plugged into the Mobicents <acronym>SLEE</acronym> server by registering with the MBean server. The kernel in that sense is only an aggregator, and not a source of actual functionality. The functionality is provided by the <acronym>SPI</acronym> implementation of the Mbeans, and in fact, all major endpoints are manageable MBeans interconnected through the MBean server. The best way to add endpoints to a Media Server is to write a new <acronym>JMX</acronym> bean which provides implementation of the endpoint's <acronym>SPI</acronym>.</para>
	<para>The <acronym>SPI</acronym> layer is an abstraction that endpoint providers must implement to enable their media-processing features. An implementation of <acronym>SPI</acronym> for an endpoint is referred to as an <emphasis>Endpoint Provider</emphasis>.</para>
<!--
An endpoint is divided logically into a Service Provider Interface (SPI) that is used to implement specific endpoint and a management interface which is used for implementing manageable resources of the endpoint. All endpoints are plugged into Mobicents SLEE server by registering with the MBean server. The kernel in that sense is only an aggregator, and not a source of actual functionality. The functionality is provided by SPI implementation of the MBeans, and in fact all major endpoints are manageable MBeans interconnected through the MBean server. The best way to add endpoints to a Media Server is to write a new JMX MBean which provides implementation of the endpoint's SPI.
 
The SPI layer is an abstraction that endpoint providers must implement to enable its media processing features. An implementation of SPI for an endpoint is referred as an Endpoint Provider
--> 
<!--Fig.2. Endpoint view.-->
	<figure
		id="figure-Endpoint.png">
		<title>EndpointManagementMBean UML diagram</title>
		<mediaobject>
			<imageobject>
				<imagedata
					align="center"
					fileref="images/Endpoint.png"
					format="PNG"
					width="700"/>
			</imageobject>
		</mediaobject>
	</figure>
<!--<para>All the endpoints management MBeans and trunk management MBeans extend the EndpointManagementMBean. The trunk is a collection of endpoints. The number of endpoint</para>
  	<remark>
All the endpoints management MBean and trunk management MBean extends the EndpointManagementMBean. The Trunk is collection of EndPoints. Number of EndPoint in Trunk is configured by Channels parameter. When the media server is deployed all the TrunkManagementMBean and EndPointManagementMBean are deployed from jboss-service.xml 
 
For example for AnnTrunkManagementMBean, corresponding AnnEndpointManagementMBean is created for 'channels' times and the service is started. When the start service is called on AnnEndpointManagementMBean, the corresponding AnnEndpointImpl is created and bound to JNDI using the media/trunk/Announcement/$, where $ is range from 1 to channel. 
 
Endpoint UML diagram
 
 
Each endpoint is extension of BaseEndpoint which implements Endpoint. 
AnnEndpointImpl : Is responsible for announcement and allows only one Connection at a time. It is used for playing media only. User can call
 
LoopEndpointImpl : This is used for testing the connection between the UA and media server. LoopEndpointImpl forwards the received media back to UA, so this is like echo test. 
 
 
The primary use of the Endpoint is to configure media resources in order to perform media processing (e.g., playing or recording voice files, speech recognition) on a call.The components of the Endpoint are depicted in Figure 2.  
 
 
 
ResourceManager UML
Each Endpoint is associated with its corresponding BaseResourceManager 
 
 
The MediaResource UML  
 
 
MediaSink : MediaSink is the base interface for objects that read media content and render the media to some destination
 
MediaSource : 
 
//TODO  Add the definition of each of the MediaResource
Resources are created upon request by a client. The resource configuration is defined by the client in its Session Descriptor and initial parameter settings are defined by the endpoint and used by the server to configure the resource correctly. The Media server  distinguishes between configuration and preparation. A resource is configured when its resources and media stream requirements are identified. It is prepared when the resources and media streams are exclusively allocated by the server and interconnected so as to satisfy the resource configuration. Only when a resource is prepared can it perform media services. The time at which resource preparation occurs affects the efficiency by which the server can manage its resources, as well as the failure modes that a client may experience.
 
The diagram shows the flow of how MediaResource are created and configured by Endpoint
MediaResource is in any of the four states describe below
 
//TODO define each of the states 
 
STATE_NULL :
 
STATE_CONFIGURED :
 
STATE_PREPARED :
 
STATE_STARTED : 
 
 
 
3.1.2 Endpoint identifiers
 
The endpoint is identified by its local name. The syntax of the local name depends on the type of endpoint being named. However, the local name for each of these types is naturally hierarchical, beginning with a term which identifies the physical   gateway containing the given endpoint and ending in a term which specifies the individual endpoint concerned.  With this in mind,  the JNDI naming rules are applied to the endpoint identifiers
</remark>-->
</section>
